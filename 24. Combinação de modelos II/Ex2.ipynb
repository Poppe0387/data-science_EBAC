{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54c0bfeb-3042-4220-a695-95c424276722",
   "metadata": {},
   "source": [
    "## GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc512ad7-bd99-489a-b8d2-de277afb8e4d",
   "metadata": {},
   "source": [
    "# 1. ***Cite 5 diferenças entre o AdaBoost e o GBM***\n",
    "\n",
    "AdaBoost e Gradient Boosting Machine (GBM) são ambos métodos de ensemble learning que combinam vários modelos base para melhorar o desempenho preditivo. Aqui estão cinco diferenças principais entre eles:\n",
    "\n",
    "###      1. Estratégia de Combinação dos Modelos:\n",
    "\n",
    "AdaBoost: Combina modelos de forma sequencial, atribuindo pesos maiores aos exemplos mal classificados em cada iteração. Cada novo modelo é treinado com foco nos exemplos que foram difíceis de classificar corretamente pelas iterações anteriores.\n",
    "GBM: Também combina modelos de forma sequencial, mas a combinação é baseada na minimização de uma função de perda. Cada novo modelo é treinado para corrigir os erros residuais dos modelos anteriores.\n",
    "\n",
    "###      2. Algoritmo de Treinamento:\n",
    "\n",
    "AdaBoost: Utiliza um processo de reponderação dos exemplos de treinamento. Os exemplos mal classificados recebem pesos mais altos para que o próximo modelo se concentre mais nesses exemplos.\n",
    "GBM: Utiliza o método de gradiente descendente para otimizar a função de perda. Cada modelo subsequente tenta minimizar o gradiente dos erros cometidos pelo modelo anterior.\n",
    "\n",
    "###     3. Sensibilidade a Ruído:\n",
    "\n",
    "AdaBoost: Pode ser bastante sensível a ruído nos dados, pois os exemplos difíceis (que podem ser ruído) recebem pesos maiores, o que pode levar o modelo a superajustar aos exemplos ruidosos.\n",
    "GBM: Também pode ser sensível a ruído, mas os métodos de regularização como o shrinkage (taxa de aprendizado) e a limitação da profundidade das árvores ajudam a mitigar o problema de overfitting.\n",
    "\n",
    "###     4. Complexidade Computacional:\n",
    "\n",
    "AdaBoost: Geralmente, é mais simples e rápido de treinar comparado ao GBM, especialmente quando usado com modelos base simples como árvores de decisão de um único nível (stumps).\n",
    "GBM: É mais complexo e computacionalmente intensivo, pois envolve a construção de árvores de decisão mais profundas e a otimização iterativa dos erros residuais.\n",
    "\n",
    "###     5. Parâmetros de Controle:\n",
    "\n",
    "AdaBoost: Tem menos parâmetros para ajustar. Os principais são o número de estimadores e o tipo de modelo base.\n",
    "GBM: Possui mais parâmetros para ajuste, como o número de estimadores, a taxa de aprendizado (shrinkage), a profundidade das árvores, o número mínimo de amostras por folha, entre outros, permitindo maior controle e potencial para melhor desempenho, mas também maior complexidade na modelagem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a63d8bc-b036-4030-907e-126925b2222d",
   "metadata": {},
   "source": [
    "# 2. Acesse o link Scikit-learn– GBM , leia a explicação (traduza se for preciso) e crie um jupyter notebook contendo o exemplo de classificação e de regress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83828948-6e6c-40aa-979f-5e7bb98481f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.913"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classificação\n",
    "\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "    max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21488bab-b409-465d-ad4d-149bea7b0d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.009154859960321"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Regressão\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n",
    "X_train, X_test = X[:200], X[200:]\n",
    "y_train, y_test = y[:200], y[200:]\n",
    "est = GradientBoostingRegressor(\n",
    "    n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0,\n",
    "    loss='squared_error'\n",
    ").fit(X_train, y_train)\n",
    "mean_squared_error(y_test, est.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ddc2a4-555a-498f-a92c-6f77b4910920",
   "metadata": {},
   "source": [
    "# 3. Cite 5 Hyperparametros importantes no GBM.\n",
    "\n",
    "### 1. n_estimators (Número de Estimadores):\n",
    "\n",
    "Define o número de árvores a serem construídas no modelo. Um número maior pode melhorar a performance até certo ponto, mas também pode levar ao sobreajuste.\n",
    "\n",
    "### 2. learning_rate (Taxa de Aprendizado):\n",
    "\n",
    "Controla o peso de cada árvore individualmente. Valores menores geralmente requerem mais árvores (n_estimators) para uma boa performance, mas podem resultar em um modelo mais robusto.\n",
    "\n",
    "### 3. max_depth (Profundidade Máxima):\n",
    "\n",
    "Limita a profundidade de cada árvore. Árvores mais profundas podem capturar relações mais complexas nos dados, mas também aumentam o risco de sobreajuste.\n",
    "\n",
    "### 4. subsample (Amostragem Subconjunto):\n",
    "\n",
    "A fração das amostras usadas para treinar cada árvore. Valores menores podem introduzir mais variabilidade e ajudar a prevenir o sobreajuste.\n",
    "\n",
    "### 5. min_samples_split e min_samples_leaf (Mínimo de Amostras para Divisão e Folha):\n",
    "\n",
    "min_samples_split especifica o número mínimo de amostras necessárias para dividir um nó.\n",
    "min_samples_leaf define o número mínimo de amostras que devem estar presentes em uma folha. Ambos ajudam a controlar o crescimento das árvores e prevenir o sobreajuste."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c4a46f-868f-4096-951e-a5def46cd954",
   "metadata": {},
   "source": [
    "# 4. Utilize o GridSearch para encontrar os melhores hyperparametros para o conjunto de dados do exemplo (load_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541dc1df-271f-4faa-868e-c5c348c20039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Carregando os dados\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Dividindo os dados em conjuntos de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Definindo o modelo\n",
    "gb_clf = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Definindo a grade de hiperparâmetros\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2, 0.5],\n",
    "    'max_depth': [1, 3, 5],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Configurando o GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=gb_clf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Ajustando o modelo\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Encontrando os melhores hiperparâmetros\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Melhores hiperparâmetros encontrados:\")\n",
    "print(best_params)\n",
    "\n",
    "# Avaliando o modelo com os melhores hiperparâmetros\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Acurácia: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb04441-ede6-4a73-adca-461508cab40e",
   "metadata": {},
   "source": [
    "# 5. Acessando o artigo do Jerome Friedman (Stochastic) e pensando no nome dado ao Stochastic GBM, qual é a maior diferença entre os dois algoritmos?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f611ff-6837-473f-bbe7-ea67caa3f0b0",
   "metadata": {},
   "source": [
    "A maior diferença entre o Gradient Boosting Machine (GBM) tradicional e o Stochastic Gradient Boosting Machine (Stochastic GBM) está na maneira como os dados são amostrados durante o treinamento de cada árvore.\n",
    "No GBM tradicional a amostragem é completa, em cada iteração, todas as amostras do conjunto de dados são utilizadas para treinar cada árvore. O modelo ajusta cada árvore para corrigir os erros das árvores anteriores usando todo o conjunto de dados disponível. Enquanto no Stochastic GBM a amostragem é aleatória (subsample), em cada iteração, apenas uma amostra aleatória do conjunto de dados é usada para treinar cada árvore. Essa amostra é uma fração do total de dados, controlada pelo hiperparâmetro subsample. Por exemplo, se subsample=0.5, apenas 50% dos dados são usados para treinar cada árvore.\n",
    "\n",
    "Sendo assim, as maiores vantagens do Stochastic GBM consistem na *redução do overfittting* (ao usar uma fração dos dados, o Stochastic GBM introduz mais variabilidade e pode ajudar a reduzir o risco de overfitting. As árvores são menos correlacionadas, o que melhora a generalização do modelo) e *eficiência computacional* (treinar cada árvore com uma amostra menor de dados pode reduzir o tempo de treinamento, especialmente em conjuntos de dados muito grandes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cfef90-fdc3-4ab8-b06d-3cb77c116940",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
